import torch
import os
from transformers import CLIPProcessor, CLIPModel

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- H√†m kh·ªüi t·∫°o v√† load model CLIP ƒë√£ train ---
def load_trained_model(checkpoint_dir):
    print("üîÑ ƒêang kh·ªüi t·∫°o model CLIP...")
    
    # 1. Load Config ƒë·ªÉ l·∫•y t√™n model g·ªëc
    config_path = os.path.join(checkpoint_dir, "config.pt")
    if os.path.exists(config_path):
        config = torch.load(config_path, map_location=DEVICE)
        model_name = config.get('model_name', "openai/clip-vit-base-patch32")
    else:
        model_name = "openai/clip-vit-base-patch32" # Fallback m·∫∑c ƒë·ªãnh
        
    print(f"   - Base model: {model_name}")

    # 2. Kh·ªüi t·∫°o Model & Processor chu·∫©n t·ª´ Hugging Face
    model = CLIPModel.from_pretrained(model_name).to(DEVICE)
    processor = CLIPProcessor.from_pretrained(model_name)
    
    # 3. Load Weights cho Projection Layers (Ph·∫ßn b·∫°n ƒë√£ train)
    # L∆∞u √Ω: C√°c file .pt ph·∫£i kh·ªõp t√™n v·ªõi l√∫c l∆∞u
    text_proj_path = os.path.join(checkpoint_dir, "text_proj.pt")
    image_proj_path = os.path.join(checkpoint_dir, "image_proj.pt")
    
    if os.path.exists(text_proj_path):
        print("   - Loading Text Projection weights...")
        model.text_projection.load_state_dict(torch.load(text_proj_path, map_location=DEVICE))
        
    if os.path.exists(image_proj_path):
        print("   - Loading Image Projection weights...")
        # Trong CLIPModel chu·∫©n, l·ªõp n√†y t√™n l√† visual_projection
        model.visual_projection.load_state_dict(torch.load(image_proj_path, map_location=DEVICE))
    
    model.eval() # Chuy·ªÉn sang ch·∫ø ƒë·ªô ƒë√°nh gi√° (kh√¥ng train)
    print("‚úÖ Model loaded successfully!")

    # Tr·∫£ v·ªÅ model v√† processor (thay th·∫ø cho tokenizer/image_processor c≈©)
    return model, processor